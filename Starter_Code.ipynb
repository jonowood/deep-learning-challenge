{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import re\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "application_df = pd.read_csv(\"Resources/charity_data.csv\")\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df = application_df.drop(columns=['EIN', 'NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "unique_counts = application_df.nunique()\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "application_counts = application_df['APPLICATION_TYPE'].value_counts()\n",
    "print(application_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "cutoff_value = 120\n",
    "application_types_to_replace = list(application_counts[application_counts <= cutoff_value].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "application_df['APPLICATION_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "classification_counts = application_df['CLASSIFICATION'].value_counts()\n",
    "print(classification_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may find it helpful to look at CLASSIFICATION value counts >1\n",
    "filtered_classification_counts = classification_counts[classification_counts > 1]\n",
    "print(filtered_classification_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "cutoff_value = 100\n",
    "classification_counts = application_df['CLASSIFICATION'].value_counts()\n",
    "classifications_to_replace = list(classification_counts[classification_counts <= cutoff_value].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df['CLASSIFICATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "application_df_converted = pd.get_dummies(application_df, columns=['APPLICATION_TYPE', 'CLASSIFICATION', 'AFFILIATION', 'USE_CASE', 'ORGANIZATION', 'INCOME_AMT', 'SPECIAL_CONSIDERATIONS'], drop_first=True)\n",
    "\n",
    "# Create the target array y by extracting the 'IS_SUCCESSFUL' column\n",
    "application_df_converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = application_df_converted['IS_SUCCESSFUL'].values\n",
    "X = application_df_converted.drop('IS_SUCCESSFUL', axis=1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "input_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Define the number of neurons for the first hidden layer\n",
    "hidden_nodes_layer1 = 64\n",
    "\n",
    "# Define the number of neurons for the second hidden layer\n",
    "hidden_nodes_layer2 = 32\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=input_features, activation='relu'))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "nn.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile the model\n",
    "# nn.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# nn.compile(optimizer='adagrad', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# nn.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# nn.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# nn.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# num_epochs = 100\n",
    "# batch_size = 32\n",
    "\n",
    "# history = nn.fit(\n",
    "#     X_train_scaled,\n",
    "#     y_train,\n",
    "#     epochs=num_epochs,\n",
    "#     batch_size=batch_size,\n",
    "#     verbose=1,\n",
    "#     validation_split=0.2\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of optimizers\n",
    "optimizers = [SGD(), Adam(), RMSprop(), Adagrad(), Adadelta(), Adamax(), Nadam()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['optimizer', 'train_loss', 'train_accuracy', 'val_loss', 'val_accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The following code adds an EarlyStopping callback with a patience of 10 epochs, meaning that the training will stop if the validation loss doesn't improve for 10 consecutive epochs. This can significantly speed up the training process without compromising the integrity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# batch_size = 32\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Loop through the optimizers\n",
    "for optimizer in optimizers:\n",
    "    # Create and compile the model with the current optimizer\n",
    "    nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    history = nn.fit(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Extract optimizer name using regex\n",
    "    optimizer_name = re.search(r\"optimizer_v2\\.(.+?)\\.\", str(optimizer)).group(1)\n",
    "    \n",
    "    # Save the results to the DataFrame\n",
    "    results_df = results_df.append({\n",
    "        'optimizer': optimizer_name,\n",
    "        'train_loss': history.history['loss'][-1],\n",
    "        'train_accuracy': history.history['accuracy'][-1],\n",
    "        'val_loss': history.history['val_loss'][-1],\n",
    "        'val_accuracy': history.history['val_accuracy'][-1]\n",
    "    }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('optimizer_results.csv',\n",
    "                  index=False,\n",
    "                  header=True,\n",
    "                  float_format='%.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a sound to let you know its done \n",
    "import winsound\n",
    "duration = 2000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side\n",
    "\n",
    "# Create a new Excel workbook and add a worksheet\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Optimizer Results\"\n",
    "\n",
    "# Write the DataFrame to the worksheet\n",
    "for r in dataframe_to_rows(results_df, index=False, header=True):\n",
    "    ws.append(r)\n",
    "\n",
    "# Apply formatting to the header row\n",
    "for cell in ws[\"1:1\"]:\n",
    "    cell.font = Font(bold=True)\n",
    "    cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "    cell.fill = PatternFill(start_color=\"FFC0CB\", end_color=\"FFC0CB\", fill_type=\"solid\")\n",
    "    cell.border = Border(left=Side(style='thin'), \n",
    "                         right=Side(style='thin'), \n",
    "                         top=Side(style='thin'), \n",
    "                         bottom=Side(style='thin'))\n",
    "\n",
    "# Apply formatting to the data cells\n",
    "for row in ws.iter_rows(min_row=2):\n",
    "    for cell in row:\n",
    "        cell.number_format = '0.0000'\n",
    "        cell.alignment = Alignment(horizontal=\"right\", vertical=\"center\")\n",
    "        cell.border = Border(left=Side(style='thin'), \n",
    "                             right=Side(style='thin'), \n",
    "                             top=Side(style='thin'), \n",
    "                             bottom=Side(style='thin'))\n",
    "\n",
    "# Adjust column widths\n",
    "for column_cells in ws.columns:\n",
    "    length = max(len(str(cell.value)) for cell in column_cells)\n",
    "    ws.column_dimensions[column_cells[0].column_letter].width = length + 2\n",
    "\n",
    "# Save the workbook to an Excel file\n",
    "wb.save('optimizer_results.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = results_df.sort_values(by='val_accuracy', ascending=False)\n",
    "sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimizer of the best-performing model\n",
    "best_optimizer = sorted_results.iloc[0]['optimizer']\n",
    "best_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best model using the best optimizer\n",
    "best_model = Sequential()\n",
    "best_model.add(Dense(units=16, activation='relu', input_dim=X_train.shape[1]))\n",
    "best_model.add(Dense(units=8, activation='relu'))\n",
    "best_model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dict = {\n",
    "    \"adam\": Adam,\n",
    "    \"sgd\": SGD,\n",
    "    \"rmsprop\": RMSprop,\n",
    "    \"adadelta\": Adadelta,\n",
    "    \"adagrad\": Adagrad,\n",
    "    \"adamax\": Adamax,\n",
    "    \"nadam\": Nadam\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optimizer_string = results_df.loc[results_df['val_accuracy'].idxmax(), 'optimizer']\n",
    "\n",
    "best_optimizer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optimizer_instance = optimizer_dict[best_optimizer_string]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the best model\n",
    "best_model.compile(optimizer=best_optimizer, loss='binary_crossentropy', metrics=['train_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "best_model.fit(X_train_scaled, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the best model to an HDF5 file\n",
    "best_model.save('AlphabetSoupCharity_Optimization.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
